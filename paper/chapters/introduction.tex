\section{Introduction}
In this paper we investigate finding a simplified closed form approximation of a multilayer perceptron (MLP): Given a function template, for example $f(x) = a \cdot x + b$ and a trained MLP, we examine different methods to find values for the parameters $a$ and $b$ such that $f$ approximates the MLP with certain error guarantees. We consider MLPs with piecewise linear activation functions and function templates, which can be encoded as a satisfiability modulo theories (SMT) formula. To do so, we focus on two different methods.
\par
First, we look at an incremental SMT approach: given a set of samples $S$ and a template, we use an SMT solver to fit the parameters of the template to the samples. In a second step, we use an SMT solver to find an outlier, meaning a sample $t \notin S$ such that the output of the MLP deviates at least a given $\epsilon$ from the function evaluated at this point. If such a sample is found, we add it to $S$ and repeat the process.\par
\par
The second, more traditional approach takes input-output pairs of the MLP and then uses existing least-squares curve fitting implementations to find parameters for the given template. Then we reuse the method to find outliers form the SMT approach to determine an upper and lower bound for the maximum difference between the MLP and the function found. Currently, only linear functions of arbitrary dimension and one-dimensional polynomials are supported for this approach.
\par
Section~\ref{sec:pre} goes through some preliminaries before we give a more detailed description our work in Section~\ref{sec:mlp}. A very short overview of the implementation can be found in Section~\ref{sec:imp}. The implementation is evaluated in Section~\ref{sec:eva}. We give a short notion on related work in Section~\ref{sec:rel} before concluding the paper in Section~\ref{sec:con}.